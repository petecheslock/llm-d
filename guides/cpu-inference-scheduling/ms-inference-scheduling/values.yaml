# Optimized CPU configuration using TinyLlama-1.1B-Chat-v1.0
# This configuration uses a 1.1B parameter instruction-tuned model with 2K context for chat/completion
# Memory allocation: 8Gi per replica (2 replicas = 16Gi total for model serving)

modelArtifacts:
  uri: "hf://TinyLlama/TinyLlama-1.1B-Chat-v1.0"
  size: 4Gi  # Model + tokenizer + config files need ~3GB, setting 4Gi for safety
  authSecretName: ""  # TinyLlama is public, no auth needed
  name: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

accelerator:
  type: cpu

# Routing configuration - required for inference-scheduling
routing:
  servicePort: 8000
  proxy:
    image: quay.io/petecheslock/llm-d-routing-sidecar:v0.4.0-rc.1-arm64
    imagePullPolicy: IfNotPresent
    connector: nixlv2
    secure: false
    # Disable zap flags since our locally built image doesn't support them
    zapEncoder: null
    zapLogLevel: null

decode:
  create: true
  replicas: 2 
  monitoring:
    podmonitor:
      enabled: true
      portName: "metrics"
      path: "/metrics"
      interval: "30s"
  containers:
  - name: "vllm"
    image: quay.io/petecheslock/llm-d-cpu:v0.4.0-arm64
    imagePullPolicy: IfNotPresent
    modelCommand: custom
    command:
      - "python3"
      - "-m"
      - "vllm.entrypoints.openai.api_server"
    args:
      - "--model"
      - "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "8200"
      - "--dtype"
      - "bfloat16"
      - "--max-model-len"
      - "2048"  # 2K context (max supported by TinyLlama)
      - "--max-num-batched-tokens"
      - "2048"  # Match max-model-len
      - "--kv-cache-memory-bytes"
      - "104857600"  # 100MB in bytes (minimal for testing)
      - "--disable-frontend-multiprocessing"
    securityContext:
      seccompProfile:
        type: Unconfined
      capabilities:
        add:
        - SYS_NICE
    env:
      - name: VLLM_TARGET_DEVICE
        value: "cpu"
      # VLLM_CPU_KVCACHE_SPACE removed - using --kv-cache-memory-bytes arg instead for finer control
      - name: VLLM_CPU_OMP_THREADS_BIND
        value: "auto"
    ports:
      - containerPort: 8200
        name: metrics
        protocol: TCP
    resources:
      limits:
        memory: 8Gi  # 2.2GB model + 100MB KV cache + 2.5GB vLLM/PyTorch + 3GB compilation overhead + 0.2GB safety margin
        cpu: "4"
      requests:
        cpu: "500m"   # Minimal request for scheduling
        memory: 3Gi    # Sufficient for model loading phase
    device: "cpu"
    mountModelVolume: true
    volumeMounts:
    - name: metrics-volume
      mountPath: /.config
    - name: torch-compile-cache
      mountPath: /.cache
    startupProbe:
      httpGet:
        path: /health
        port: 8200
      initialDelaySeconds: 300  # 5 minutes - increased for slow CPU compilation
      periodSeconds: 30
      timeoutSeconds: 10
      failureThreshold: 80  # Increased to allow more time
    livenessProbe:
      httpGet:
        path: /health
        port: 8200
      periodSeconds: 15
      timeoutSeconds: 10
      failureThreshold: 3
    readinessProbe:
      httpGet:
        path: /v1/models
        port: 8200
      periodSeconds: 10
      timeoutSeconds: 5
      failureThreshold: 5
  volumes:
    - name: metrics-volume
      emptyDir: {}
    - name: torch-compile-cache
      emptyDir: {}

# Disable prefill for simple CPU example
prefill:
  create: false

# When true, use LeaderWorkerSet for multi-node CPU setups
multinode: false
