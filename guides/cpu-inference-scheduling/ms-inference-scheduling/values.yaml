# Optimized CPU configuration for Apple Silicon MacBook using Qwen2-0.5B-Instruct
# This configuration uses a small instruction-tuned model for chat/completion

modelArtifacts:
  uri: "hf://Qwen/Qwen2-0.5B-Instruct"
  size: 2Gi
  authSecretName: ""  # Qwen2 is public, no auth needed
  name: "Qwen/Qwen2-0.5B-Instruct"

accelerator:
  type: cpu

# Routing configuration - required for inference-scheduling
routing:
  servicePort: 8000
  proxy:
    image: localhost/llm-d-routing-sidecar:v0.4.0-rc.1-arm64
    imagePullPolicy: IfNotPresent
    connector: nixlv2
    secure: false
    # Disable zap flags since our locally built image doesn't support them
    zapEncoder: null
    zapLogLevel: null

decode:
  create: true
  replicas: 2 
  monitoring:
    podmonitor:
      enabled: true
      portName: "metrics"
      path: "/metrics"
      interval: "30s"
  containers:
  - name: "vllm"
    image: quay.io/rh_ee_micyang/vllm-service:macos
    imagePullPolicy: IfNotPresent
    modelCommand: custom
    command:
      - "/bin/bash"
      - "-c"
      - |
        source ${HOME}/vllm_env/bin/activate
        python3 -m vllm.entrypoints.openai.api_server \
          --model Qwen/Qwen2-0.5B-Instruct \
          --host 0.0.0.0 \
          --port 8200 \
          --dtype bfloat16 \
          --max-model-len 2048 \
          --disable-frontend-multiprocessing
    securityContext:
      seccompProfile:
        type: Unconfined
      capabilities:
        add:
        - SYS_NICE
    env:
      - name: VLLM_TARGET_DEVICE
        value: "cpu"
      - name: VLLM_PLATFORM
        value: "cpu"
      - name: CUDA_VISIBLE_DEVICES
        value: ""
      - name: VLLM_PORT
        value: "8200"
      - name: VLLM_CPU_NUM_OF_RESERVED_CPU
        value: "1"
      - name: VLLM_CPU_KVCACHE_SPACE
        value: "2"
    ports:
      - containerPort: 8200
        name: metrics
        protocol: TCP
    resources:
      limits:
        memory: 8Gi
        cpu: "4"
      requests:
        cpu: "500m"   # Minimal request for scheduling
        memory: 1Gi   # Minimal memory request
    device: "cpu"
    mountModelVolume: true
    volumeMounts:
    - name: metrics-volume
      mountPath: /.config
    - name: torch-compile-cache
      mountPath: /.cache
    startupProbe:
      httpGet:
        path: /health
        port: 8200
      initialDelaySeconds: 300
      periodSeconds: 30
      timeoutSeconds: 10
      failureThreshold: 80
    livenessProbe:
      httpGet:
        path: /health
        port: 8200
      periodSeconds: 15
      timeoutSeconds: 10
      failureThreshold: 3
    readinessProbe:
      httpGet:
        path: /v1/models
        port: 8200
      periodSeconds: 10
      timeoutSeconds: 5
      failureThreshold: 5
  volumes:
    - name: metrics-volume
      emptyDir: {}
    - name: torch-compile-cache
      emptyDir: {}

# Disable prefill for simple CPU example
prefill:
  create: false

# When true, use LeaderWorkerSet for multi-node CPU setups
multinode: false
