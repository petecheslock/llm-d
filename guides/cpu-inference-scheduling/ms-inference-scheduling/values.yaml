# Optimized CPU configuration using SmolLM2-360M-Instruct
# This configuration uses a small instruction-tuned model with 4K context for chat/completion
# Memory allocation: 6Gi per replica (2 replicas = 12Gi total for model serving)

modelArtifacts:
  uri: "hf://HuggingFaceTB/SmolLM2-360M-Instruct"
  size: 1Gi
  authSecretName: ""  # SmolLM2 is public, no auth needed
  name: "HuggingFaceTB/SmolLM2-360M-Instruct"

accelerator:
  type: cpu

# Routing configuration - required for inference-scheduling
routing:
  servicePort: 8000
  proxy:
    image: quay.io/petecheslock/llm-d-routing-sidecar:v0.4.0-rc.1-arm64
    imagePullPolicy: IfNotPresent
    connector: nixlv2
    secure: false
    # Disable zap flags since our locally built image doesn't support them
    zapEncoder: null
    zapLogLevel: null

decode:
  create: true
  replicas: 2 
  monitoring:
    podmonitor:
      enabled: true
      portName: "metrics"
      path: "/metrics"
      interval: "30s"
  containers:
  - name: "vllm"
    image: quay.io/petecheslock/llm-d-cpu:v0.4.0-arm64
    imagePullPolicy: IfNotPresent
    modelCommand: custom
    command:
      - "python3"
      - "-m"
      - "vllm.entrypoints.openai.api_server"
    args:
      - "--model"
      - "HuggingFaceTB/SmolLM2-360M-Instruct"
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "8200"
      - "--dtype"
      - "bfloat16"
      - "--max-model-len"
      - "4096"
      - "--max-num-batched-tokens"
      - "4096"
      - "--disable-frontend-multiprocessing"
    securityContext:
      seccompProfile:
        type: Unconfined
      capabilities:
        add:
        - SYS_NICE
    env:
      - name: VLLM_TARGET_DEVICE
        value: "cpu"
      - name: VLLM_CPU_KVCACHE_SPACE
        value: "1"  # Reduced from 2 to 1GB to reduce memory pressure
      - name: VLLM_CPU_OMP_THREADS_BIND
        value: "auto"
    ports:
      - containerPort: 8200
        name: metrics
        protocol: TCP
    resources:
      limits:
        memory: 6Gi  # Increased: 1.5GB model + 2GB KV cache + 2.5GB overhead/safety margin
        cpu: "4"
      requests:
        cpu: "500m"   # Minimal request for scheduling
        memory: 2Gi    # Increased to ensure scheduling with sufficient memory
    device: "cpu"
    mountModelVolume: true
    volumeMounts:
    - name: metrics-volume
      mountPath: /.config
    - name: torch-compile-cache
      mountPath: /.cache
    startupProbe:
      httpGet:
        path: /health
        port: 8200
      initialDelaySeconds: 180
      periodSeconds: 30
      timeoutSeconds: 10
      failureThreshold: 60
    livenessProbe:
      httpGet:
        path: /health
        port: 8200
      periodSeconds: 15
      timeoutSeconds: 10
      failureThreshold: 3
    readinessProbe:
      httpGet:
        path: /v1/models
        port: 8200
      periodSeconds: 10
      timeoutSeconds: 5
      failureThreshold: 5
  volumes:
    - name: metrics-volume
      emptyDir: {}
    - name: torch-compile-cache
      emptyDir: {}

# Disable prefill for simple CPU example
prefill:
  create: false

# When true, use LeaderWorkerSet for multi-node CPU setups
multinode: false
